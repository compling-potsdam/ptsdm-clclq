## Show and Tell: Learning to Connect Images and Text for Natural Communication

*Malihe Alikhani, University of Pittsburgh*

*June 16th, 2021*

### Abstract

From the gestures that accompany speech to images in social media
posts, humans effortlessly combine words with visual presentations.
However, machines are not equipped to understand and generate such
presentations due to peopleâ€™s pervasive reliance on commonsense and
world knowledge in relating words and images. I present a novel framework
for modeling and learning a deeper combined understanding of text and
images by classifying inferential relations to predict temporal, causal, and
logical entailments in context. This enables systems to make inferences
with high accuracy while revealing author expectations and social-context
preferences. I proceed to design methods for generating text based on
visual input that use these inferences to provide users with key requested
information. The results show a dramatic improvement in the consistency
and quality of the generated text by decreasing spurious information by
half. Finally, I sketch my other projects on human-robot collaboration and
conversational systems and describe my research vision: to build human-level 
communicative systems and grounded artificial intelligence by leveraging the cognitive science of language use.
